# Fine-tune with Axolotl (example config)
base_model: Qwen/Qwen2.5-Coder-7B
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer

datasets:
  - path: data/optimized_train_dataset.jsonl
    type: alpaca
    field_instruction: instruction
    field_input: input
    field_output: output

output_dir: ./swiftui-finetuned-model

# Training hyperparameters
epochs: 3
micro_batch_size: 8
per_device_train_batch_size: 8
gradient_accumulation_steps: 4
learning_rate: 2e-4
weight_decay: 0.01
warmup_steps: 100
fp16: true
max_grad_norm: 1.0
save_steps: 500
logging_steps: 50

# LoRA / PEFT configuration
lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj

optimizer: adamw_torch_fused

peft:
  method: lora

misc:
  seed: 42
  dataloader_num_workers: 4
...
