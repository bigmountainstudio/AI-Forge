# Dev config for smoke test (1-epoch, small subset)
base_model: gpt2
model_type: GPT2LMHeadModel
tokenizer_type: GPT2TokenizerFast

datasets:
  - path: data/dev_subset_100.jsonl
    type: alpaca
    field_instruction: instruction
    field_input: input
    field_output: output

output_dir: ./swiftui-finetuned-model-dev

# Dev training hyperparameters (fast smoke test)
epochs: 1
micro_batch_size: 8
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 2e-4
weight_decay: 0.01
warmup_steps: 10
fp16: true
max_grad_norm: 1.0
save_steps: 100
logging_steps: 10

# LoRA / PEFT configuration (same as main)
lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj

peft:
  method: lora

misc:
  seed: 42
  dataloader_num_workers: 2
